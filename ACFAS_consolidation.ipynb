{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPeKK0e7owaLiuee3H9gtkM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancisLa/ACFAS/blob/main/ACFAS_consolidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV6_oTlu37-8"
      },
      "source": [
        "#Consolidation of ACFAS dataframe\n",
        "This is where we explain what this code do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6nzMmra71Gs"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9NVOY8E7imG"
      },
      "source": [
        "##Import standard libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fewb7sgc5RFH"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import pandas as pd\n",
        "import importlib\n",
        "import pkg_resources\n",
        "import types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import non standard libraries"
      ],
      "metadata": {
        "id": "-L4Ru3MmJYiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def import_or_install(package, version=None):\n",
        "    try:\n",
        "        if version is None:\n",
        "            importlib.import_module(package)\n",
        "        else:\n",
        "            dist = pkg_resources.get_distribution(package)\n",
        "            assert dist.version == version, f\"{package}=={version} not found\"\n",
        "    except (ImportError, pkg_resources.DistributionNotFound, AssertionError):\n",
        "        if version is None:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        else:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{package}=={version}\"])\n",
        "    finally:\n",
        "        globals()[package] = importlib.import_module(package)\n",
        "\n",
        "import_or_install('platform')\n",
        "import_or_install('socket')\n",
        "import_or_install('uuid')\n",
        "import_or_install('psutil')\n",
        "import_or_install('GPUtil')"
      ],
      "metadata": {
        "id": "47UWnWZ1_Ut8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GGBK5r95i5u"
      },
      "source": [
        "##Project name, path and authorization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This code uses Google drive to access the data. The project_name will be used as the main directory in Google drive (inside Colab data). It can be run locally (hosted = False) or through Colab hosting (hosted = True)."
      ],
      "metadata": {
        "id": "nn1BHlZcuYje"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S1_g9wwkHM8"
      },
      "source": [
        "project_name = 'ACFAS'\n",
        "operation_name = 'consolidation'\n",
        "hosted = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guwwvAgMd_0E"
      },
      "source": [
        "###Colab hosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puFEJJxR5vxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14aafcb5-a855-464d-adde-249ad393413e"
      },
      "source": [
        "if hosted:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive',force_remount=True)\n",
        "  project_dir = '/content/gdrive/My Drive/Colab Data/' + project_name\n",
        "  tools_dir = '/content/gdrive/My Drive/Colab Data/Tools'\n",
        "  operation_dir = tools_dir = '/content/gdrive/My Drive/Colab Data/'+ project_name+'/'+ operation_name\n",
        "  os.makedirs(operation_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUtnDHVZeJGS"
      },
      "source": [
        "###Local hosting\n",
        "Local runtime need some preparation (https://research.google.com/colaboratory/local-runtimes.html)\n",
        "\n",
        "It might be safer to work with python 3.7.9 (https://python.org/downloads/release/python-379/)\n",
        "\n",
        "You may need to run this command line :\n",
        "\n",
        "jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=9090 --no-browser\n",
        "\n",
        "You may use google drive by installing gdrive locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtLSr9g2ePT2"
      },
      "source": [
        "if not hosted:\n",
        "  try:\n",
        "    os.chdir(os.path.join('G:',os.sep,'Mon disque'))\n",
        "  except:\n",
        "    os.chdir(os.path.join('G:',os.sep,'My Drive'))\n",
        "  project_dir = os.path.join('Colab Data',project_name)\n",
        "  operation_dir = os.path.join('Colab Data',project_name,operation_name)\n",
        "  tools_dir = os.path.join('Colab Data','Tools')\n",
        "  os.makedirs(operation_dir, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install requirements"
      ],
      "metadata": {
        "id": "U08ziPuB0Cvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have a text file named *project_name_operation_name_requirements.txt* in *operation_dir*, with package and version on each line ('re==', or 'pandas==2.0.3'), this code will automatically show them."
      ],
      "metadata": {
        "id": "7y0n2Rhiwf9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.isfile(os.path.join(operation_dir, project_name+'_'+operation_name+'_requirements.txt')):\n",
        "  with open(os.path.join(operation_dir, project_name+'_'+operation_name+'_requirements.txt'), 'r') as file_:\n",
        "    for line in file_:\n",
        "      package, version = line.strip().split('==')\n",
        "      if version != '':\n",
        "        print(f\"Package {package}, version {version}\")\n",
        "      else:\n",
        "        #print(f\"Package {package}\")\n",
        "        pass"
      ],
      "metadata": {
        "id": "e4VIsduO0PZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5EOc99o6nNv"
      },
      "source": [
        "#Import data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialisation des valeurs constantes qui contiennent les chemins des documents sources et document cible.\n",
        "Le document source est une table à deux dimentions dont les noms des colonnes sont dans la 1e ligne."
      ],
      "metadata": {
        "id": "xcPTuvktH4-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constantes de programme\n",
        "PATH_SOURCE = '' # entrer le chemin du fichier à ingérer\n",
        "BALISEATOMISATION = '{Atomisation incomplète}' # spécifier la balise d'incomplétude pour cette ingestion. Lorsque ce texte se retrouve dans un champs ingéré,\n",
        "                    #   il signifi que l'atomisation est incomplète et les valeurs dans la cellule ont besoin d'être extraites à une étape ultérieure.\n",
        "SEPARATEUR = \"$\" # séparateur sert à marquer les différents champs concatenés à l'ingestion.\n",
        "CHARGEMENT = '3'"
      ],
      "metadata": {
        "id": "t4rLOA86JMKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour générer un hash MD5\n",
        "import hashlib\n",
        "def generate_md5_hash(value):\n",
        "    return hashlib.md5(value.encode('utf-8')).hexdigest()"
      ],
      "metadata": {
        "id": "3hl5qUg9JS0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Bloc 2 - table Programme\n",
        "\n",
        "def process_file(file_path, df_cible):\n",
        "    # Check the file extension and read the file accordingly\n",
        "    if file_path.endswith('.csv'):\n",
        "        df_source = pd.read_csv(file_path)\n",
        "        print(\"le fichier csv\",file_path,\" sera traité\")\n",
        "    elif file_path.endswith('.xlsx'):\n",
        "        df_source = pd.read_excel(file_path)\n",
        "        print(\"le fichier xlsx\", file_path,\" sera traité\")\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n",
        "\n",
        "    #display(df_source)\n",
        "    #charger le document cible qui contient les noms de toutes les colonnes\n",
        "    # voir Variables&Lexique.xlsx\n",
        "\n",
        "\n",
        "    # transfert des énoncés sources dans leurs tables respectives\n",
        "    # décommenter la ligne puis ajouté le nom de la source après =\n",
        "    #Inscrit la date de l'ingestion  dans le dataset\n",
        "    x=datetime.now()\n",
        "    df_cible['Date_Ingestion']=x\n",
        "\n",
        "    # Transfert de la date du congrès\n",
        "    df_cible['Temporalite_Date'] = df_source['jour']\n",
        "    # df_cible['_Temporalité_Clé]\n",
        "    # df_cible['_Temporalité_Id]\n",
        "    df_cible['Temporalite_EnonceSource']=df_source['notice_complete']\n",
        "    df_cible['Temporalite_Analyste']='Table des programme compilée par Luc Gauvreau et Julien Vallières Gingras'\n",
        "    df_cible['Temporalite_ReferenceBiblio']=\"{1933-1962 : Programme du [no congrès] congrès, [année du congrès]/1963-2001 : Programme général : [no congrès] congrès annuel, [année du congrès]}\"\n",
        "\n",
        "\n",
        "    # # transfert des élément de l'évènement\n",
        "    # df_cible['Evenement_CleEvenement']\n",
        "    # df_cible['Evenement_IdEvenement']\n",
        "    df_cible['Evenement_TitreEvenement']=df_source['titre']\n",
        "    # df_cible['Evenement_CleTemporaliteDebEvenement']\n",
        "    # df_cible['Evenement_CleTemporaliteFinEvenement']\n",
        "    # df_cible['Evenement_SeqDistinctiveEvenement']\n",
        "    df_cible['Evenement_DtHrChargement1']=CHARGEMENT\n",
        "    # df_cible['Evenement_CleEvenementSource']\n",
        "    df_cible['Evenement_TitreLongEvenement']=df_source['titre']\n",
        "    df_cible['Evenement_ResumeEvenement']=df_source['resume']\n",
        "    df_cible['Evenement_DescEvenement']=df_source['description']\n",
        "    df_cible['Evenement_Analyste']='Table des programme compilée par Luc Gauvreau et Julien Vallières Gingras'\n",
        "    df_cible['Evenement_EnonceSource']=df_source['notice_complete']\n",
        "    df_cible['Evenement_ReferenceBiblio']=\"{1933-1962 : Programme du [no congrès] congrès, [année du congrès]/1963-2001 : Programme général : [no congrès] congrès annuel, [année du congrès]}\"\n",
        "\n",
        "\n",
        "    #transfert des valeurs dans la table Peronnes\n",
        "    df_cible[\"Personne_AliasPersonne\"] = df_source['auteur']\n",
        "    df_cible['Personne_AutoratEnonce'] = df_source['auteur']\n",
        "    # df_cible['Personne_ClePersonne']\n",
        "    # df_cible['Personne_IdPersonne']\n",
        "    df_cible['Personne_PrenomPersonne'] = BALISEATOMISATION+df_source['auteur']\n",
        "    df_cible['Personne_DeuxiemePrenomPersonne']=BALISEATOMISATION+df_source['auteur']\n",
        "    df_cible['Personne_NomFamillePersonne']=BALISEATOMISATION+df_source['auteur']\n",
        "    df_cible['Personne_TitrePersonne']=BALISEATOMISATION+df_source['auteur']\n",
        "    # df_cible['Personne_AnneeNaissancePersonne']\n",
        "    # df_cible['Personne_AnneeDecesPersonne']\n",
        "    # df_cible['Personne_SeqDistinctivePersonne']\n",
        "    df_cible['Personne_DtHrChargement1']=CHARGEMENT\n",
        "    df_cible['Personne_PersonneRoles']= BALISEATOMISATION+df_source['personne_roles']\n",
        "    # df_cible['Personne_CleEvenementSource')\n",
        "    df_cible['Personne_EnonceSource'] = df_source['notice_complete']\n",
        "    df_cible['Personne_Analyste']='Table des programme compilée par Luc Gauvreau et Julien Vallières Gingras'\n",
        "    df_cible['Personne_ReferenceBiblio']=\"{1933-1962 : Programme du [no congrès] congrès, [année du congrès]/1963-2001 : Programme général : [no congrès] congrès annuel, [année du congrès]}\"\n",
        "\n",
        "\n",
        "    #transfert vers la table Collectivité\n",
        "    # df_cible['Collectivite_CleCollectivite']\n",
        "    # df_cible['Collectivite_IdCollectivite']\n",
        "    df_cible['Collectivite_NomCollectivite'] = BALISEATOMISATION+df_source['auteur']\n",
        "    # df_cible['Collectivite_DescCollectivite']\n",
        "    df_cible['Collectivite_AliasCollectivite'] = BALISEATOMISATION+df_source['auteur']\n",
        "    df_cible['Collectivite_AutoratEnonce'] = df_source['auteur']\n",
        "    # df_cible['Collectivite_AnneeFondationCollectivite']\n",
        "    # df_cible['Collectivite_AnneeDissolutionCollectivite']\n",
        "    # df_cible['Collectivite_SeqDistinctiveCollectivite0']\n",
        "    df_cible['Collectivite_DtHrChargement1']=CHARGEMENT\n",
        "    # df_cible['Collectivite_CleEvenementSource']\n",
        "    df_cible['Collectivite_Analyste'] = \"compilé par Julien Vallières et Luc Gauvreau\"\n",
        "    df_cible['Collectivite_ReferenceBiblio'] = \"{1933-1962 : Programme du [no congrès] congrès, [année du congrès]/1963-2001 : Programme général : [no congrès] congrès annuel, [année du congrès]}\"\n",
        "    df_cible['Collectivite_EnonceSource'] = df_source['notice_complete']\n",
        "\n",
        "\n",
        "    #Transfert vers la table Lieu\n",
        "    # df_cible['Lieu_CleLieu']\n",
        "    # df_cible['Lieu_IdLieu']=\n",
        "    # df_cible['Lieu_SeqDistinctiveLieu']\n",
        "    df_cible['Lieu_DtHrChargement1']=CHARGEMENT\n",
        "    # df_cible['Lieu_CleEvenementSource']\n",
        "    # df_cible['Lieu_DescLieu']\n",
        "    df_cible['Lieu_NomLongLieu'] = '{extraction de df_source_auteur}'+df_source['organisateur']\n",
        "    df_cible['Lieu_EnonceSource'] = df_source['notice_complete']\n",
        "    # df_cible['Lieu_CoordonneesKMLEpicentre']\n",
        "    # df_cible['Lieu_PolygoneKMLZone']\n",
        "    df_cible['Lieu_Analyste'] = \"compilé par Julien Vallières et Luc Gauvreau\"\n",
        "    df_cible['Lieu_ReferenceBiblio'] = \"{1933-1962 : Programme du [no congrès] congrès, [année du congrès]/1963-2001 : Programme général : [no congrès] congrès annuel, [année du congrès]}\"\n",
        "\n",
        "\n",
        "    #Transfert vers la table Document Source\n",
        "    #Transfert vers la table Sujet\n",
        "    df_cible['Sujet_TitreSujet']=df_source['seance_sujet']\n",
        "    # df_cible['Sujet_CleSujet]\n",
        "    # df_cible['Sujet_IdSujet]\n",
        "    # df_cible['Sujet_SeqDistinctiveSujet]\n",
        "    df_cible['Sujet_DtHrChargement1']=CHARGEMENT\n",
        "    # df_cible['Sujet_CleEvenementSource]\n",
        "    df_cible['Sujet_EnonceSource'] = df_source['notice_complete']\n",
        "    # df_cible['Sujet_TitreLongSujet]\n",
        "    # df_cible['Sujet_DescSujet]=df_source['']\n",
        "    df_cible['Sujet_Analyste'] = \"compilé par Julien Vallières et Luc Gauvreau\"\n",
        "    df_cible['Sujet_ReferenceBiblio'] = \"{1933-1962 : Programme du [no congrès] congrès, [année du congrès]/1963-2001 : Programme général : [no congrès] congrès annuel, [année du congrès]}\"\n",
        "\n",
        "    #Inscrit la date de l'ingestion dans le dataset\n",
        "    x=datetime.now()\n",
        "    df_cible['Date_Ingestion']=x\n",
        "\n",
        "    #display(df_cible)\n",
        "\n",
        "    return(df_cible)"
      ],
      "metadata": {
        "id": "QcCYxwCDJYoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_definition_file_path =  os.path.join(project_dir,\"Ingestion_all\",r\"Table_Enonce_Initialise.xlsx\")\n",
        "source_data_file_path =  os.path.join(project_dir,\"Ingestion_all\",r\"input/programme.csv\")\n",
        "df_cible = pd.read_excel(column_definition_file_path)\n",
        "DF = process_file(source_data_file_path, df_cible)\n",
        "#display(DF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFg8AaNDJi6R",
        "outputId": "e1275a30-6bd3-4d17-e26c-efb0962ae13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "le fichier csv /content/gdrive/My Drive/Colab Data/ACFAS/Ingestion_all/input/programme.csv  sera traité\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-848abb4f1c0f>:6: DtypeWarning: Columns (0,4,5,13,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_source = pd.read_csv(file_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hgrT2KPFrbg"
      },
      "source": [
        "#Export data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Ft1DC1akfe"
      },
      "source": [
        "##System information"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note on what type of machine this code was executed."
      ],
      "metadata": {
        "id": "VrYYt-mUHYou"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-mRBjnJansa"
      },
      "source": [
        "info={}\n",
        "info['time'] = datetime.now()\n",
        "info['platform'] = platform.system()\n",
        "info['platform-release'] = platform.release()\n",
        "info['platform-version'] = platform.version()\n",
        "info['architecture'] = platform.machine()\n",
        "info['hostname'] = socket.gethostname()\n",
        "info['ip-address'] = socket.gethostbyname(socket.gethostname())\n",
        "info['mac-address'] = ':'.join(re.findall('..', '%012x' % uuid.getnode()))\n",
        "info['processor'] = platform.processor()\n",
        "info['threads'] = str(psutil.cpu_count()) + ' logical cores'\n",
        "try:\n",
        "  info['speed'] = str(round(psutil.cpu_freq().current)) + \" Mhz (currently)\"\n",
        "except:\n",
        "  info['speed'] = 'unknown'\n",
        "info['ram'] = str(round(psutil.virtual_memory().total / (1024.0 **3)))+' Go'\n",
        "info['disk'] = str(round(psutil.disk_usage('/').total / (1024.0 **3)))+' Go'\n",
        "try:\n",
        "  info['gpu_total_ram'] = str(round(GPUtil.getGPUs()[0].memoryTotal / 1024.0))+' Go'\n",
        "except:\n",
        "  info['gpu_total_ram'] = 'unknown'\n",
        "try:\n",
        "  info['gpu_free_ram'] = str(round(GPUtil.getGPUs()[0].memoryFree / 1024.0))+' Go'\n",
        "except:\n",
        "  info['gpu_free_ram'] = 'unknown'\n",
        "try:\n",
        "  info['gpu_name'] = GPUtil.getGPUs()[0].name\n",
        "except:\n",
        "  info['gpu_name'] = 'unknown'\n",
        "DF_info = pd.DataFrame.from_dict(info, orient='index', columns=['Value'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mkccVI5YXlD"
      },
      "source": [
        "##Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note the packages currently imported into the working environment."
      ],
      "metadata": {
        "id": "2uDgO_bRHvY7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n9dFjQMYh9Z"
      },
      "source": [
        "module_names = set()\n",
        "for name, val in list(globals().items()):  # Create a copy of items to avoid iteration issues\n",
        "    if isinstance(val, types.ModuleType):\n",
        "        module_names.add(val.__name__)\n",
        "\n",
        "filepath=os.path.join(operation_dir, project_name+'_'+operation_name+'_requirements.txt')\n",
        "with open(filepath, 'w') as file_:\n",
        "  for name in module_names:\n",
        "    try:\n",
        "      version = pkg_resources.get_distribution(name).version\n",
        "    except:\n",
        "      version = \"\"\n",
        "    file_.write(f\"{name}=={version}\\n\")\n",
        "\n",
        "DF_requirements = pd.read_csv(filepath, sep=\"\\=\\=| @ \", header=None, engine = 'python')\n",
        "DF_requirements.columns=['pack.','ver.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7z0mda-ZRjG"
      },
      "source": [
        "##Export dataframe to excel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming the data is in panda dataframe form, let's export it to excel for easy visualization. Let's add the system information and requirements."
      ],
      "metadata": {
        "id": "UYu5GTZcITDe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbUMiJU0FzFJ"
      },
      "source": [
        "writer = pd.ExcelWriter(os.path.join(operation_dir, project_name+'_'+operation_name+'.xlsx'))\n",
        "DF_info.to_excel(writer,'System')\n",
        "DF_requirements.to_excel(writer,'Module')\n",
        "DF.to_excel(writer,'Data')\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Export dataframe to pickle"
      ],
      "metadata": {
        "id": "RRpMcP-omfMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming the data is in panda dataframe form, let's export it with pickle for easy manipulation."
      ],
      "metadata": {
        "id": "gs8GaEhSJCqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.to_pickle(DF,\n",
        "             os.path.join(operation_dir, project_name+'_'+operation_name+'.pkl'))"
      ],
      "metadata": {
        "id": "aNlHD8Agmri3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}